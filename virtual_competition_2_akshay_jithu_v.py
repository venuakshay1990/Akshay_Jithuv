# -*- coding: utf-8 -*-
"""Virtual_competition_2_Akshay_jithu_V.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NKlB9n-TPppuDOBgDyOGJGvozmmtpeEb

Loading the required Libaries:
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, recall_score, f1_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

"""loading the datasets"""

train_data = pd.read_csv('/content/train_ctrUa4K.csv')
test_data = pd.read_csv('/content/test_lAUu6dG.csv')

"""loading head data of both datasets"""

train_data.head()

test_data.head()

"""loading tail data of both datasets"""

train_data.tail()

test_data.tail()

"""Understanding the columns"""

train_data.info()

test_data.info()

train_data.dtypes

test_data.dtypes

train_data.isnull().sum()

test_data.isnull().sum()

train_data.columns

test_data.columns

train_data.describe()

test_data.describe()

"""Finding out how many unique values each columns contain:-  """

train_nunique_values_all = train_data.nunique()
test_nunique_values_all = test_data.nunique()

train_nunique_values_all

test_nunique_values_all

train_data.dtypes

test_data.dtypes

sns.countplot(x='Loan_Status',data=train_data)

"""# **clearly its evident that loan_status acceptence is higher than disapproval.**

Start of data preprocessing
"""

train_data.drop('Loan_ID',axis=1,inplace=True)
test_data.drop('Loan_ID',axis=1,inplace=True)

train_data.head()

test_data.head()

"""Replacing null values
1. mode on all object dtypes of both train and test datasets
"""

object_columns = train_data.select_dtypes(include='object').columns[train_data.select_dtypes(include='object').isnull().any()]
for col in object_columns:
  mode_value = train_data[col].mode()[0]
  train_data[col].fillna(mode_value,inplace=True)

object_columns2 = test_data.select_dtypes(include='object').columns[test_data.select_dtypes(include='object').isnull().any()]
for col in object_columns2:
  mode_value = train_data[col].mode()[0]
  test_data[col].fillna(mode_value,inplace=True)

"""Now we have to check whether we should apply mean or median to fill the remaining columns with integer or float dtype.

I used loan amount calculation as median filling
loan amount term as filling 360 for all missing values
forward filling for credit histroy.
"""

train_data.dtypes

test_data.dtypes

train_data.LoanAmount.skew()

plt.hist(train_data['LoanAmount'])
plt.show()

train_data.Loan_Amount_Term.skew()

plt.hist(train_data['Loan_Amount_Term'])
plt.show()

train_data.Credit_History.skew()

plt.hist(train_data['Credit_History'])
plt.show()

train_data['LoanAmount'].fillna(train_data['LoanAmount'].median(), inplace=True)
train_data['Loan_Amount_Term'].fillna(360.0,inplace=True)
train_data['Credit_History'].fillna(method='ffill', inplace=True)

train_data.isnull().sum()

test_data.LoanAmount.skew()

plt.hist(test_data['LoanAmount'])
plt.show()

test_data.Loan_Amount_Term.skew()

plt.hist(test_data['Loan_Amount_Term'])
plt.show()

test_data.Credit_History.skew()

plt.hist(test_data['Credit_History'])
plt.show()

test_data['LoanAmount'].fillna(test_data['LoanAmount'].median(), inplace=True)
test_data['Loan_Amount_Term'].fillna(360.0,inplace=True)
test_data['Credit_History'].fillna(method='ffill', inplace=True)

test_data.isnull().sum()

"""# **OUTLIERS DETECTION**"""

train_data.info()

test_data.info()

train_data.dtypes

train_data.ApplicantIncome.skew()

plt.hist(train_data['ApplicantIncome'])
plt.show()

train_data.CoapplicantIncome.skew()

plt.hist(train_data['CoapplicantIncome'])
plt.show()

train_data.LoanAmount.skew()

plt.hist(train_data['LoanAmount'])
plt.show()

train_data.Loan_Amount_Term.skew()

plt.hist(train_data['Loan_Amount_Term'])
plt.show()

train_data.Credit_History.skew()

plt.hist(train_data['Credit_History'])
plt.show()



"""# **Since the data is skewed we apply IQR outlier test**"""

new_columns1 = ['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History']

for i in new_columns1:
  plt.figure(i)
  plt.boxplot(train_data[i])
  plt.title(i);

new_columns2 = ['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History']

for j in new_columns2:
    Q1=np.percentile(train_data[j],25,method='midpoint')
    Q2=np.percentile(train_data[j],50,method='midpoint')
    Q3=np.percentile(train_data[j],75,method='midpoint')
    IQR = Q3-Q1
    low_lim = Q1-1.5*IQR
    up_lim = Q3+1.5*IQR
    outliers = []
    for h in train_data[j]:
        if ((h < low_lim) | (h > up_lim)):
            outliers.append(h)
    train_data[j]=train_data[j].clip(lower=low_lim,upper=up_lim)

new_columns3 = ['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History']
for i in new_columns3:
  plt.figure(i)
  plt.boxplot(train_data[i])
  plt.title(i);

train_data.info()

"""NOW DOING OULIER DETECTION ON TEST_DATA"""

test_data.ApplicantIncome.skew()

plt.hist(test_data['ApplicantIncome'])
plt.show()

test_data.CoapplicantIncome.skew()

plt.hist(test_data['CoapplicantIncome'])
plt.show()

test_data.LoanAmount.skew()

plt.hist(test_data['LoanAmount'])
plt.show()

test_data.Loan_Amount_Term.skew()

plt.hist(test_data['Loan_Amount_Term'])
plt.show()

test_data.Credit_History.skew()

plt.hist(test_data['Credit_History'])
plt.show()

"""# **Applying IQR TEST on test set as the values are skewed.**"""

new_columns4 = ['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History']
for i in new_columns4:
  plt.figure(i)
  plt.boxplot(test_data[i])
  plt.title(i);

new_columns5 = ['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History']
for j in new_columns5:
    Q1=np.percentile(test_data[j],25,method='midpoint')
    Q2=np.percentile(test_data[j],50,method='midpoint')
    Q3=np.percentile(test_data[j],75,method='midpoint')
    IQR = Q3-Q1
    low_lim = Q1-1.5*IQR
    up_lim = Q3+1.5*IQR
    outliers = []
    for h in train_data[j]:
        if ((h < low_lim) | (h > up_lim)):
            outliers.append(h)
    test_data[j]=test_data[j].clip(lower=low_lim,upper=up_lim)

new_columns6 = ['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History']
for i in new_columns6:
  plt.figure(i)
  plt.boxplot(test_data[i])
  plt.title(i);

test_data.info()

train_data.corr()

sns.heatmap(train_data.corr(),cmap="YlGnBu",annot=True)

test_data.corr()

sns.heatmap(test_data.corr(),cmap="YlGnBu",annot=True)

"""THE NEXT STEP IS ENCODING. CONVERTING ALL OBJECT DATA TO VALUES THAT THE MACHINE CAN UNDERSTAND the data. FOR THAT

# WE USE **ONE HOT ENCODING**
"""

train_data.info()

test_data.info()

"""# **NOW LETS SEPERATE FEATURES AND THE TARGET VARIABLE**"""

X_train = train_data.drop('Loan_Status',axis=1)
y_train = train_data['Loan_Status']
X_test = test_data.copy()

X_train_encode = pd.get_dummies(X_train)
X_test_encode = pd.get_dummies(X_test)

X_train_encode.head()

X_test_encode.head()

"""Now scaling is done"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_encode)
X_train_scaled = pd.DataFrame(X_train_encode)

X_train_scaled.describe()

scaler = StandardScaler()
X_test_scaled = scaler.fit_transform(X_test_encode)
X_test_scaled = pd.DataFrame(X_test_encode)

X_test_scaled.describe()

"""Next step is to do the modelling Using
**1.Logistic Regression,**
**2.KNN,**
**3.SVM,**
**4.Decision Tree,**
**5.Random Forest**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

models = {
    "Logistic Regression": LogisticRegression(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier()
}

print("\nModel Training and Evaluation:")
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred_train = model.predict(X_train_scaled)
    accuracy_train = accuracy_score(y_train, y_pred_train)
    print(f"{name} Accuracy on Train Data: {accuracy_train}")

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred_train = model.predict(X_train_scaled)
    accuracy_train = accuracy_score(y_train, y_pred_train)
    print(f"{name} Accuracy on Train Data: {accuracy_train}")
 # Calculate precision, recall, F1 score, and confusion matrix
    print(f"\n{name} Metrics:")
    print(classification_report(y_train, y_pred_train))
    print(f"Confusion Matrix:\n{confusion_matrix(y_train, y_pred_train)}")

from sklearn.model_selection import train_test_split, GridSearchCV

"""Since it provided max accuracy for decision tree and random forest.
# I am taking **Random forest as best model**

# Step 4: Fine Tuning
"""

# (Hyperparameter tuning for Random Forest)
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier()
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train_scaled, y_train)

print("\nBest Parameters for Random Forest:", grid_search.best_params_)

"""# **Making predictions using the fine-tuned Random Forest model**"""

best_rf = grid_search.best_estimator_
predictions = best_rf.predict(X_test_scaled)

"""# **Step 5 : Adding sample_submission file to the work worksheet**

Steps include:-

1.loading the sample_submission file into colab notebook.

2.Replace the columns with the predictions values.

3.save the updated dataset as a .csv file.
"""

sample_submission = pd.read_csv('/content/sample_submission_49d68Cx.csv')

sample_submission.head(15)

sample_submission.tail(15)

sample_submission['Loan_Status'] = predictions

sample_submission.head()

sample_submission.tail()

"""IT is evident that there are predicteed changes are reflected in the sample_submission workbook."""

# Save the updated dataset as a CSV file
sample_submission.to_csv('Updated_submission.csv', index=False)

sample_submission.head()